{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.358\n",
      "Model:                            OLS   Adj. R-squared:                  0.355\n",
      "Method:                 Least Squares   F-statistic:                     141.8\n",
      "Date:                Wed, 09 Feb 2022   Prob (F-statistic):          1.06e-119\n",
      "Time:                        16:00:23   Log-Likelihood:                -1276.6\n",
      "No. Observations:                1279   AIC:                             2565.\n",
      "Df Residuals:                    1273   BIC:                             2596.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "const                    3.0105      0.226     13.298      0.000       2.566       3.455\n",
      "volatile acidity        -1.2105      0.107    -11.337      0.000      -1.420      -1.001\n",
      "chlorides               -1.7655      0.433     -4.082      0.000      -2.614      -0.917\n",
      "total sulfur dioxide    -0.0022      0.001     -3.859      0.000      -0.003      -0.001\n",
      "sulphates                0.8949      0.120      7.439      0.000       0.659       1.131\n",
      "alcohol                  0.2827      0.019     15.207      0.000       0.246       0.319\n",
      "==============================================================================\n",
      "Omnibus:                       14.372   Durbin-Watson:                   2.100\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               18.599\n",
      "Skew:                          -0.144   Prob(JB):                     9.15e-05\n",
      "Kurtosis:                       3.515   Cond. No.                     1.39e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.39e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "Root Mean Squared Error: 0.6259157889490522\n",
      "-- Epoch 1\n",
      "Norm: 0.21, NNZs: 5, Bias: 0.411615, T: 1279, Avg. loss: 0.024799\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.22, NNZs: 5, Bias: 0.419031, T: 2558, Avg. loss: 0.012343\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.23, NNZs: 5, Bias: 0.421117, T: 3837, Avg. loss: 0.012010\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.24, NNZs: 5, Bias: 0.420584, T: 5116, Avg. loss: 0.011754\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.25, NNZs: 5, Bias: 0.423752, T: 6395, Avg. loss: 0.011535\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.25, NNZs: 5, Bias: 0.423470, T: 7674, Avg. loss: 0.011346\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.26, NNZs: 5, Bias: 0.423014, T: 8953, Avg. loss: 0.011176\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 7 epochs took 0.00 seconds\n",
      "\n",
      "***SGD=\n",
      "Root Mean Squared Error: 0.6808846340906913\n",
      "\n",
      "***Ridge Regression Coefficients ** alpha=0\n",
      "3.0105263238850744\n",
      "[-1.21046416 -1.7655295  -0.00217522  0.89491841  0.28268935]\n",
      "Root Mean Squared Error: 0.6259157889490529\n",
      "\n",
      "***Ridge Regression Coefficients ** alpha=0.16\n",
      "3.3324060288920516\n",
      "[-1.09689194 -1.55787263 -0.00209533  0.79561428  0.25018895]\n",
      "Root Mean Squared Error: 0.6234564185902967\n",
      "\n",
      "***Ridge Regression Coefficients ** alpha=0.17\n",
      "3.349750965754922\n",
      "[-1.09042919 -1.5465499  -0.00208941  0.79016659  0.24841788]\n",
      "Root Mean Squared Error: 0.6234451149243397\n",
      "\n",
      "***Ridge Regression Coefficients ** alpha=0.18\n",
      "3.3668203389238816\n",
      "[-1.08403594 -1.53539511 -0.00208343  0.78479603  0.24667303]\n",
      "Root Mean Squared Error: 0.6234468121961954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/var/folders/wl/bvgg9vxx64scj9r0jxzqssj00000gn/T/ipykernel_45815/3517919560.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lassoreg.fit(X_train, y_train)\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+02, tolerance: 8.583e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/var/folders/wl/bvgg9vxx64scj9r0jxzqssj00000gn/T/ipykernel_45815/3517919560.py:109: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  model.fit(X_train, y_train)\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.723e+02, tolerance: 8.583e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/var/folders/wl/bvgg9vxx64scj9r0jxzqssj00000gn/T/ipykernel_45815/3517919560.py:109: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  model.fit(X_train, y_train)\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.723e+02, tolerance: 8.583e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/var/folders/wl/bvgg9vxx64scj9r0jxzqssj00000gn/T/ipykernel_45815/3517919560.py:109: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  model.fit(X_train, y_train)\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.723e+02, tolerance: 8.583e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/var/folders/wl/bvgg9vxx64scj9r0jxzqssj00000gn/T/ipykernel_45815/3517919560.py:109: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  model.fit(X_train, y_train)\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.723e+02, tolerance: 8.583e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/var/folders/wl/bvgg9vxx64scj9r0jxzqssj00000gn/T/ipykernel_45815/3517919560.py:109: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  model.fit(X_train, y_train)\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.723e+02, tolerance: 8.583e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***Lasso Regression Coefficients ** alpha=0\n",
      "3.0105263238850517\n",
      "[-1.21046416 -1.7655295  -0.00217522  0.89491841  0.28268935]\n",
      "Root Mean Squared Error: 0.625915788949053\n",
      "\n",
      "***Lasso Regression Coefficients ** alpha=0.1\n",
      "5.64659890539484\n",
      "[-0. -0. -0.  0.  0.]\n",
      "Root Mean Squared Error: 0.7584549718351333\n",
      "\n",
      "***Lasso Regression Coefficients ** alpha=0.5\n",
      "5.64659890539484\n",
      "[-0. -0. -0.  0.  0.]\n",
      "Root Mean Squared Error: 0.7584549718351333\n",
      "\n",
      "***Lasso Regression Coefficients ** alpha=1\n",
      "5.64659890539484\n",
      "[-0. -0. -0.  0.  0.]\n",
      "Root Mean Squared Error: 0.7584549718351333\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0 l1ratio=0\n",
      "34.99871062872156\n",
      "[ 4.12835075e-02 -1.14952802e+00 -1.77927063e-01  2.78700036e-02\n",
      " -1.87340739e+00  2.68362616e-03 -2.77748370e-03 -3.15166657e+01\n",
      " -2.54486051e-01  9.24040106e-01  2.67797417e-01]\n",
      "Root Mean Squared Error: 0.6200574149384267\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0 l1ratio=0.25\n",
      "34.99871062872156\n",
      "[ 4.12835075e-02 -1.14952802e+00 -1.77927063e-01  2.78700036e-02\n",
      " -1.87340739e+00  2.68362616e-03 -2.77748370e-03 -3.15166657e+01\n",
      " -2.54486051e-01  9.24040106e-01  2.67797417e-01]\n",
      "Root Mean Squared Error: 0.6200574149384267\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0 l1ratio=0.5\n",
      "34.99871062872156\n",
      "[ 4.12835075e-02 -1.14952802e+00 -1.77927063e-01  2.78700036e-02\n",
      " -1.87340739e+00  2.68362616e-03 -2.77748370e-03 -3.15166657e+01\n",
      " -2.54486051e-01  9.24040106e-01  2.67797417e-01]\n",
      "Root Mean Squared Error: 0.6200574149384267\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0 l1ratio=0.75\n",
      "34.99871062872156\n",
      "[ 4.12835075e-02 -1.14952802e+00 -1.77927063e-01  2.78700036e-02\n",
      " -1.87340739e+00  2.68362616e-03 -2.77748370e-03 -3.15166657e+01\n",
      " -2.54486051e-01  9.24040106e-01  2.67797417e-01]\n",
      "Root Mean Squared Error: 0.6200574149384267\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0 l1ratio=1\n",
      "34.99871062872156\n",
      "[ 4.12835075e-02 -1.14952802e+00 -1.77927063e-01  2.78700036e-02\n",
      " -1.87340739e+00  2.68362616e-03 -2.77748370e-03 -3.15166657e+01\n",
      " -2.54486051e-01  9.24040106e-01  2.67797417e-01]\n",
      "Root Mean Squared Error: 0.6200574149384267\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=1e-05 l1ratio=0\n",
      "5.79235704869118\n",
      "[ 0.01354955 -1.17238946 -0.17934619  0.01553758 -1.91891827  0.00297445\n",
      " -0.00286783 -1.69924137 -0.40359536  0.88119123  0.29527512]\n",
      "Root Mean Squared Error: 0.6186555552369347\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=1e-05 l1ratio=0.25\n",
      "6.00486892285003\n",
      "[ 0.01373261 -1.17207469 -0.17902343  0.01562753 -1.91999493  0.00297298\n",
      " -0.00286761 -1.91599808 -0.40248437  0.88161442  0.29505256]\n",
      "Root Mean Squared Error: 0.6186645847484629\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=1e-05 l1ratio=0.5\n",
      "6.407996646756606\n",
      "[ 0.01409745 -1.17161296 -0.17869495  0.01579779 -1.92069922  0.0029696\n",
      " -0.00286679 -2.32739865 -0.40039197  0.88230813  0.29465138]\n",
      "Root Mean Squared Error: 0.6186803463635624\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=1e-05 l1ratio=0.75\n",
      "7.467721743402258\n",
      "[ 1.50886274e-02 -1.17064629e+00 -1.78348412e-01  1.62446945e-02\n",
      " -1.92010846e+00  2.95962797e-03 -2.86387202e-03 -3.40927731e+00\n",
      " -3.94917851e-01  8.83932460e-01  2.93635082e-01]\n",
      "Root Mean Squared Error: 0.6187197824224765\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=1e-05 l1ratio=1\n",
      "17.85468054038457\n",
      "[ 2.49779607e-02 -1.16251269e+00 -1.77753273e-01  2.06213921e-02\n",
      " -1.90105672e+00  2.85591709e-03 -2.83126445e-03 -1.40155962e+01\n",
      " -3.41400270e-01  8.98770220e-01  2.83881445e-01]\n",
      "Root Mean Squared Error: 0.6191454068745069\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.0001 l1ratio=0\n",
      "4.254369147604175\n",
      "[ 0.0131604  -1.17677933 -0.18446474  0.0146558  -1.81799147  0.00296839\n",
      " -0.00285091 -0.18345151 -0.40023102  0.86643585  0.29771752]\n",
      "Root Mean Squared Error: 0.6185613598693035\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.0001 l1ratio=0.25\n",
      "4.076145084156323\n",
      "[ 0.01282045 -1.17560094 -0.18154759  0.01458143 -1.8310644   0.00297594\n",
      " -0.00285541 -0.         -0.40077841  0.86717508  0.29767201]\n",
      "Root Mean Squared Error: 0.6185693700338325\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.0001 l1ratio=0.5\n",
      "4.077779192009241\n",
      "[ 0.01264634 -1.17424271 -0.17857078  0.01458341 -1.84421396  0.0029819\n",
      " -0.00285947 -0.         -0.40044201  0.86821041  0.29745371]\n",
      "Root Mean Squared Error: 0.6185837603546813\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.0001 l1ratio=0.75\n",
      "4.079584194819657\n",
      "[ 0.01246724 -1.17284314 -0.17553903  0.01458603 -1.85779042  0.00298804\n",
      " -0.00286365 -0.         -0.40013798  0.86928831  0.29723075]\n",
      "Root Mean Squared Error: 0.618598651159192\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.0001 l1ratio=1\n",
      "4.08157279543199\n",
      "[ 0.01228276 -1.17139949 -0.17244907  0.01458933 -1.87181677  0.00299439\n",
      " -0.00286797 -0.         -0.39986902  0.87041114  0.29700288]\n",
      "Root Mean Squared Error: 0.6186140641455287\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.001 l1ratio=0\n",
      "3.750999187805069\n",
      "[ 0.01871215 -1.17597574 -0.19515302  0.01275127 -1.18603663  0.00291877\n",
      " -0.0027459  -0.02191403 -0.33022754  0.78202539  0.30426653]\n",
      "Root Mean Squared Error: 0.6186230986509528\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.001 l1ratio=0.25\n",
      "3.6971887835114763\n",
      "[ 0.01820366 -1.17195054 -0.17744592  0.01257229 -1.20639478  0.00293438\n",
      " -0.00275588 -0.         -0.31729217  0.78091491  0.30326985]\n",
      "Root Mean Squared Error: 0.6187511645472554\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.001 l1ratio=0.5\n",
      "3.6649596693421174\n",
      "[ 0.01765623 -1.16723724 -0.15852947  0.01240574 -1.23179119  0.00295226\n",
      " -0.00276739 -0.         -0.30404033  0.78026513  0.30217561]\n",
      "Root Mean Squared Error: 0.618889481720807\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.001 l1ratio=0.75\n",
      "3.6337514010496674\n",
      "[ 0.01702129 -1.16162162 -0.13810736  0.0122459  -1.26446062  0.00297386\n",
      " -0.00278124 -0.         -0.29075357  0.78026903  0.30097962]\n",
      "Root Mean Squared Error: 0.6190366156330854\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.001 l1ratio=1\n",
      "3.6046707452027142\n",
      "[ 0.01626774 -1.15480121 -0.11575397  0.01209725 -1.30781464  0.00299991\n",
      " -0.00279814 -0.         -0.27762697  0.78126182  0.29964431]\n",
      "Root Mean Squared Error: 0.6191936282963523\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.01 l1ratio=0\n",
      "2.96281817909193\n",
      "[ 0.0248009  -0.90311047 -0.00991496  0.00671778 -0.26896603  0.00376948\n",
      " -0.00286842 -0.00340709 -0.18190618  0.55111427  0.31511091]\n",
      "Root Mean Squared Error: 0.619732479961025\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.01 l1ratio=0.25\n",
      "2.5743182663196347\n",
      "[ 0.03011923 -0.9144444  -0.          0.00502741 -0.05385138  0.00359964\n",
      " -0.00275648 -0.         -0.06977843  0.50768012  0.31363392]\n",
      "Root Mean Squared Error: 0.6214618411645152\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.01 l1ratio=0.5\n",
      "2.3579121467915387\n",
      "[ 0.03387944 -0.92442306  0.          0.00376742 -0.          0.00348747\n",
      " -0.00267609 -0.         -0.          0.47166676  0.31168149]\n",
      "Root Mean Squared Error: 0.6225075422100831\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.01 l1ratio=0.75\n",
      "2.399795554834623\n",
      "[ 0.03376311 -0.92983445  0.          0.00261143 -0.          0.00346175\n",
      " -0.00265075 -0.         -0.          0.43098774  0.31081099]\n",
      "Root Mean Squared Error: 0.6227464520998066\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.01 l1ratio=1\n",
      "2.446120103095825\n",
      "[ 0.0337344  -0.93756694  0.          0.00142522 -0.          0.00344929\n",
      " -0.00262388 -0.         -0.          0.382792    0.31001788]\n",
      "Root Mean Squared Error: 0.6231265097918763\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.18 l1ratio=0\n",
      "2.281633014317294\n",
      "[ 0.05324536 -0.18836858  0.06658426  0.00151068 -0.01397229  0.00572995\n",
      " -0.00336053 -0.0004795  -0.03238133  0.12431113  0.29693656]\n",
      "Root Mean Squared Error: 0.6350061534048599\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.18 l1ratio=0.25\n",
      "2.377588305121682\n",
      "[ 0.0527671  -0.          0.          0.         -0.          0.00507606\n",
      " -0.00332361 -0.         -0.          0.          0.27888212]\n",
      "Root Mean Squared Error: 0.6474721451229181\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.18 l1ratio=0.5\n",
      "2.830516226627566\n",
      "[ 0.03700754 -0.          0.          0.         -0.          0.00407897\n",
      " -0.00336733 -0.         -0.          0.          0.24973974]\n",
      "Root Mean Squared Error: 0.6521421354034819\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.18 l1ratio=0.75\n",
      "3.3144839216713624\n",
      "[ 0.02065709 -0.          0.          0.         -0.          0.00308531\n",
      " -0.00343151 -0.         -0.          0.          0.21817712]\n",
      "Root Mean Squared Error: 0.6598273849202521\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.18 l1ratio=1\n",
      "3.833134743091616\n",
      "[ 0.00367751 -0.          0.          0.         -0.          0.00210213\n",
      " -0.00351997 -0.         -0.          0.          0.18388147]\n",
      "Root Mean Squared Error: 0.6709813289084092\n",
      "Best RMSE 0.6185613598693035 Best alpha: 0.0001  Best l1 ratio: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.727e+02, tolerance: 8.583e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.730e+02, tolerance: 8.583e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+02, tolerance: 8.583e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.870e+02, tolerance: 8.583e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.242e+02, tolerance: 8.583e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn                 import metrics\n",
    "import statsmodels.api       as sm\n",
    "import numpy as np\n",
    "PATH     = \"../datasets/\"\n",
    "CSV_DATA = \"winequality.csv\"\n",
    "dataset  = pd.read_csv(PATH + CSV_DATA,\n",
    "                       skiprows=1,       # Don't include header row as part of data.\n",
    "                       encoding = \"ISO-8859-1\", sep=',',\n",
    "                       names=('fixed acidity', 'volatile acidity', 'citric acid',\n",
    "                              'residual sugar', 'chlorides', 'free sulfur dioxide',\n",
    "                              'total sulfur dioxide', 'density', 'pH', 'sulphates',\n",
    "                              'alcohol', 'quality'))\n",
    "\n",
    "X = dataset[['volatile acidity', 'chlorides', 'total sulfur dioxide', 'sulphates',\n",
    "             'alcohol']]\n",
    "\n",
    "# Adding an intercept *** This is requried ***. Don't forget this step.\n",
    "# The intercept centers the error residuals around zero\n",
    "# which helps to avoid over-fitting.\n",
    "X_withConst = sm.add_constant(X)\n",
    "y = dataset['quality'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_withConst, y,\n",
    "                                                    test_size=0.2, random_state=0)\n",
    "\n",
    "def performLinearRegression(X_train, X_test, y_train, y_test):\n",
    "    model = sm.OLS(y_train, X_train).fit()\n",
    "    predictions = model.predict(X_test) # make the predictions by the model\n",
    "    print(model.summary())\n",
    "    print('Root Mean Squared Error:',\n",
    "          np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
    "    return predictions\n",
    "\n",
    "predictions = performLinearRegression(X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "def performSGD(X_train, X_test, y_train, y_test, scalerY):\n",
    "    sgd = SGDRegressor(verbose=1)\n",
    "    sgd.fit(X_train, y_train)\n",
    "    print(\"\\n***SGD=\")\n",
    "    predictions = sgd.predict(X_test)\n",
    "    #print(predictions)\n",
    "\n",
    "    y_test_unscaled =  scalerY.inverse_transform(y_test)\n",
    "    predictions_unscaled = scalerY.inverse_transform(predictions.reshape(-1, 1) )\n",
    "    #print(predictions_unscaled)\n",
    "\n",
    "    print('Root Mean Squared Error:',\n",
    "          np.sqrt(metrics.mean_squared_error(y_test_unscaled,\n",
    "                                             predictions_unscaled)))\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scalerX = MinMaxScaler()\n",
    "scalerX.fit(X)\n",
    "x2Scaled = scalerX.transform(X)\n",
    "\n",
    "scalerY  = MinMaxScaler()\n",
    "reshapedY = y.reshape(-1,1)\n",
    "scalerY.fit(reshapedY)\n",
    "yScaled = scalerY.transform(reshapedY)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x2Scaled, yScaled,\n",
    "                                                    test_size=0.2, random_state=0)\n",
    "performSGD(X_train, X_test, y_train, y_test, scalerY)\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "def ridge_regression(X_train, X_test, y_train, y_test, alpha):\n",
    "    # Fit the model\n",
    "    ridgereg = Ridge(alpha=alpha, normalize=True)\n",
    "    ridgereg.fit(X_train, y_train)\n",
    "    y_pred = ridgereg.predict(X_test)\n",
    "    # predictions = scalerY.inverse_transform(y_pred.reshape(-1,1))\n",
    "    print(\"\\n***Ridge Regression Coefficients ** alpha=\" + str(alpha))\n",
    "    print(ridgereg.intercept_)\n",
    "    print(ridgereg.coef_)\n",
    "    print('Root Mean Squared Error:',\n",
    "          np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2, random_state=0)\n",
    "alphaValues = [0,  0.16, 0.17, 0.18]\n",
    "for i in range(0, len(alphaValues)):\n",
    "    ridge_regression(X_train, X_test, y_train, y_test,\n",
    "                     alphaValues[i])\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "def performLassorRegression(X_train, X_test, y_train, y_test, alpha):\n",
    "    lassoreg = Lasso(alpha=alpha, normalize=True, max_iter=1e5)\n",
    "    lassoreg.fit(X_train, y_train)\n",
    "    y_pred = lassoreg.predict(X_test)\n",
    "    print(\"\\n***Lasso Regression Coefficients ** alpha=\" + str(alpha))\n",
    "    print(lassoreg.intercept_)\n",
    "    print(lassoreg.coef_)\n",
    "    print('Root Mean Squared Error:',\n",
    "          np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "alphaValues = [0, 0.1, 0.5, 1]\n",
    "for i in range(0, len(alphaValues)):\n",
    "    performLassorRegression(X_train, X_test, y_train, y_test,\n",
    "                            alphaValues[i])\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "bestRMSE = 100000.03\n",
    "def performElasticNetRegression(X_train, X_test, y_train, y_test, alpha, l1ratio, bestRMSE,\n",
    "                                bestAlpha, bestL1Ratio):\n",
    "    model = ElasticNet(alpha=alpha, l1_ratio=l1ratio)\n",
    "    # fit model\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"\\n***ElasticNet Regression Coefficients ** alpha=\" + str(alpha)\n",
    "          + \" l1ratio=\" + str(l1ratio))\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "    print(model.intercept_)\n",
    "    print(model.coef_)\n",
    "    try:\n",
    "        if(rmse < bestRMSE):\n",
    "            bestRMSE = rmse\n",
    "            bestAlpha = alpha\n",
    "            bestL1Ratio = l1ratio\n",
    "        print('Root Mean Squared Error:', rmse)\n",
    "    except:\n",
    "        print(\"rmse =\" + str(rmse))\n",
    "\n",
    "    return bestRMSE, bestAlpha, bestL1Ratio\n",
    "\n",
    "X_elastic = dataset[['fixed acidity', 'volatile acidity', 'citric acid',\n",
    "                     'residual sugar', 'chlorides', 'free sulfur dioxide',\n",
    "                     'total sulfur dioxide', 'density', 'pH', 'sulphates',\n",
    "                     'alcohol']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_elastic, y,\n",
    "                                                    test_size=0.2, random_state=0)\n",
    "alphaValues = [0, 0.00001, 0.0001, 0.001, 0.01, 0.18]\n",
    "l1ratioValues = [0, 0.25, 0.5, 0.75, 1]\n",
    "bestAlpha   = 0\n",
    "bestL1Ratio = 0\n",
    "\n",
    "for i in range(0, len(alphaValues)):\n",
    "    for j in range(0, len(l1ratioValues)):\n",
    "        bestRMSE, bestAlpha, bestL1Ratio = performElasticNetRegression(\n",
    "            X_train, X_test, y_train, y_test,\n",
    "            alphaValues[i], l1ratioValues[j], bestRMSE,\n",
    "            bestAlpha, bestL1Ratio)\n",
    "\n",
    "print(\"Best RMSE \" + str(bestRMSE) + \" Best alpha: \" + str(bestAlpha)\n",
    "      + \"  \" + \"Best l1 ratio: \" + str(bestL1Ratio))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Coefficients: \n",
      "\n",
      "Intercept: \n",
      "[-5.16965979]\n",
      "[[ 0.12107723  0.02607586 -0.01850055  0.00493978 -0.00103087  0.05028589\n",
      "   0.57904396  0.002377  ]]\n",
      "\n",
      "Accuracy:  0.7480314960629921\n",
      "\n",
      "Confusion Matrix\n",
      "Predicted    0   1\n",
      "Actual            \n",
      "0          138   9\n",
      "1           55  52\n",
      "Accuracy: 0.5787401574803149 Num degrees: 0\n",
      "Accuracy: 0.7047244094488189 Num degrees: 1\n",
      "Accuracy: 0.7047244094488189 Num degrees: 2\n",
      "Accuracy: 0.7086614173228346 Num degrees: 3\n",
      "Accuracy: 0.7007874015748031 Num degrees: 4\n",
      "Accuracy: 0.7047244094488189 Num degrees: 5\n"
     ]
    }
   ],
   "source": [
    "import  pandas as pd\n",
    "from    sklearn.model_selection import train_test_split\n",
    "PATH    = \"../datasets/\"\n",
    "from   sklearn.linear_model    import LogisticRegression\n",
    "from   sklearn                 import metrics\n",
    "import numpy as np\n",
    "\n",
    "# load the dataset\n",
    "df = pd.read_csv(PATH + 'diabetes.csv', sep=',')\n",
    "# split into input (X) and output (y) variables\n",
    "\n",
    "X = df[['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI',\n",
    "        'DiabetesPedigreeFunction',    'Age']]\n",
    "y = df[['Outcome']]\n",
    "# Split into train and test data sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33)\n",
    "\n",
    "# Perform logistic regression.\n",
    "logisticModel = LogisticRegression(fit_intercept=True, random_state = 0,\n",
    "                                   solver='liblinear')\n",
    "logisticModel.fit(X_train,y_train)\n",
    "y_pred=logisticModel.predict(X_test)\n",
    "\n",
    "# Show model coefficients and intercept.\n",
    "print(\"\\nModel Coefficients: \")\n",
    "print(\"\\nIntercept: \")\n",
    "print(logisticModel.intercept_)\n",
    "\n",
    "print(logisticModel.coef_)\n",
    "\n",
    "# Show confusion matrix and accuracy scores.\n",
    "confusion_matrix = pd.crosstab(np.array(y_test['Outcome']), y_pred,\n",
    "                               rownames=['Actual'],\n",
    "                               colnames=['Predicted'])\n",
    "\n",
    "print('\\nAccuracy: ',metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix)\n",
    "\n",
    "# Suppress the data convergence warning.\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "from sklearn import svm\n",
    "def buildSVMmodel(degree):\n",
    "    # Create a svm Classifier using one of the following options:\n",
    "    # linear, polynomial, and radial\n",
    "    clf = svm.SVC(kernel='poly', degree=degree)\n",
    "\n",
    "    # Train the model using the training set.\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model.\n",
    "    y_pred = clf.predict(X_test)\n",
    "    from sklearn import metrics\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy: \" + str(accuracy) + \" Num degrees: \" + str(degree))\n",
    "\n",
    "degrees = [0, 1,2,3,4,5]\n",
    "for i in range(0, len(degrees)):\n",
    "    buildSVMmodel(degrees[i])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.707\n",
      "Model:                            OLS   Adj. R-squared:                  0.681\n",
      "Method:                 Least Squares   F-statistic:                     27.32\n",
      "Date:                Wed, 09 Feb 2022   Prob (F-statistic):           3.53e-09\n",
      "Time:                        16:38:42   Log-Likelihood:                -211.58\n",
      "No. Observations:                  38   AIC:                             431.2\n",
      "Df Residuals:                      34   BIC:                             437.7\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================================\n",
      "                                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------\n",
      "const                          288.6119    173.511      1.663      0.105     -64.004     641.228\n",
      "Petrol_tax                     -29.3087     11.149     -2.629      0.013     -51.967      -6.651\n",
      "Average_income                  -0.0708      0.020     -3.478      0.001      -0.112      -0.029\n",
      "Population_Driver_licence(%)  1430.5650    199.565      7.168      0.000    1025.001    1836.129\n",
      "==============================================================================\n",
      "Omnibus:                       17.327   Durbin-Watson:                   2.075\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               22.574\n",
      "Skew:                           1.347   Prob(JB):                     1.25e-05\n",
      "Kurtosis:                       5.647   Cond. No.                     9.89e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 9.89e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "Root Mean Squared Error: 62.88023441227515\n",
      "\n",
      "***Ridge Regression Coefficients ** alpha=0\n",
      "288.6119448185815\n",
      "[-2.93087031e+01 -7.08395205e-02  1.43056503e+03]\n",
      "Root Mean Squared Error: 62.880234412276394\n",
      "\n",
      "***Ridge Regression Coefficients ** alpha=0.16\n",
      "345.32347709573145\n",
      "[-2.79282706e+01 -5.98484543e-02  1.23165414e+03]\n",
      "Root Mean Squared Error: 60.76305901802675\n",
      "\n",
      "***Ridge Regression Coefficients ** alpha=0.17\n",
      "348.1492188423007\n",
      "[-2.78287536e+01 -5.92756939e-02  1.22114820e+03]\n",
      "Root Mean Squared Error: 60.69209123273996\n",
      "\n",
      "***Ridge Regression Coefficients ** alpha=0.18\n",
      "350.9082572256302\n",
      "[-2.77284341e+01 -5.87139820e-02  1.21083003e+03]\n",
      "Root Mean Squared Error: 60.626642015949194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * n_samples. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn                 import metrics\n",
    "import statsmodels.api       as sm\n",
    "import numpy as np\n",
    "PATH     = \"../datasets/\"\n",
    "CSV_DATA = \"petrol_consumption.csv\"\n",
    "dataset  = pd.read_csv(PATH + CSV_DATA)\n",
    "#   Petrol_Consumption\n",
    "X = dataset[['Petrol_tax','Average_income', 'Population_Driver_licence(%)']]\n",
    "\n",
    "# Adding an intercept *** This is requried ***. Don't forget this step.\n",
    "# The intercept centers the error residuals around zero\n",
    "# which helps to avoid over-fitting.\n",
    "X_withConst = sm.add_constant(X)\n",
    "y = dataset['Petrol_Consumption'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_withConst, y,\n",
    "                                                    test_size=0.2, random_state=0)\n",
    "\n",
    "def performLinearRegression(X_train, X_test, y_train, y_test):\n",
    "    model = sm.OLS(y_train, X_train).fit()\n",
    "    predictions = model.predict(X_test) # make the predictions by the model\n",
    "    print(model.summary())\n",
    "    print('Root Mean Squared Error:',\n",
    "          np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
    "    return predictions\n",
    "\n",
    "predictions = performLinearRegression(X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "def ridge_regression(X_train, X_test, y_train, y_test, alpha):\n",
    "    # Fit the model\n",
    "    ridgereg = Ridge(alpha=alpha, normalize=True)\n",
    "    ridgereg.fit(X_train, y_train)\n",
    "    y_pred = ridgereg.predict(X_test)\n",
    "    # predictions = scalerY.inverse_transform(y_pred.reshape(-1,1))\n",
    "    print(\"\\n***Ridge Regression Coefficients ** alpha=\" + str(alpha))\n",
    "    print(ridgereg.intercept_)\n",
    "    print(ridgereg.coef_)\n",
    "    print('Root Mean Squared Error:',\n",
    "          np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2, random_state=0)\n",
    "alphaValues = [0,  0.16, 0.17, 0.18]\n",
    "for i in range(0, len(alphaValues)):\n",
    "    ridge_regression(X_train, X_test, y_train, y_test,\n",
    "                     alphaValues[i])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.707\n",
      "Model:                            OLS   Adj. R-squared:                  0.681\n",
      "Method:                 Least Squares   F-statistic:                     27.32\n",
      "Date:                Wed, 09 Feb 2022   Prob (F-statistic):           3.53e-09\n",
      "Time:                        16:37:12   Log-Likelihood:                -211.58\n",
      "No. Observations:                  38   AIC:                             431.2\n",
      "Df Residuals:                      34   BIC:                             437.7\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================================\n",
      "                                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------\n",
      "const                          288.6119    173.511      1.663      0.105     -64.004     641.228\n",
      "Petrol_tax                     -29.3087     11.149     -2.629      0.013     -51.967      -6.651\n",
      "Average_income                  -0.0708      0.020     -3.478      0.001      -0.112      -0.029\n",
      "Population_Driver_licence(%)  1430.5650    199.565      7.168      0.000    1025.001    1836.129\n",
      "==============================================================================\n",
      "Omnibus:                       17.327   Durbin-Watson:                   2.075\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               22.574\n",
      "Skew:                           1.347   Prob(JB):                     1.25e-05\n",
      "Kurtosis:                       5.647   Cond. No.                     9.89e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 9.89e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "Root Mean Squared Error: 62.88023441227515\n",
      "\n",
      "***Lasso Regression Coefficients ** alpha=0\n",
      "288.61194481858115\n",
      "[ 0.00000000e+00 -2.93087031e+01 -7.08395205e-02  1.43056503e+03]\n",
      "Root Mean Squared Error: 62.880234412276444\n",
      "\n",
      "***Lasso Regression Coefficients ** alpha=0.1\n",
      "285.88562156788316\n",
      "[ 0.00000000e+00 -2.88715973e+01 -6.96642931e-02  1.42070740e+03]\n",
      "Root Mean Squared Error: 62.688745369720266\n",
      "\n",
      "***Lasso Regression Coefficients ** alpha=0.5\n",
      "274.97726841981535\n",
      "[ 0.00000000e+00 -2.71227890e+01 -6.49636018e-02  1.38127873e+03]\n",
      "Root Mean Squared Error: 62.02090437251334\n",
      "\n",
      "***Lasso Regression Coefficients ** alpha=1\n",
      "261.34182698473035\n",
      "[ 0.00000000e+00 -2.49367788e+01 -5.90877376e-02  1.33199289e+03]\n",
      "Root Mean Squared Error: 61.412477040601885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/var/folders/wl/bvgg9vxx64scj9r0jxzqssj00000gn/T/ipykernel_45815/3872961118.py:34: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lassoreg.fit(X_train, y_train)\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.627e+04, tolerance: 5.203e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn                 import metrics\n",
    "import statsmodels.api       as sm\n",
    "import numpy as np\n",
    "PATH     = \"../datasets/\"\n",
    "CSV_DATA = \"petrol_consumption.csv\"\n",
    "dataset  = pd.read_csv(PATH + CSV_DATA)\n",
    "#   Petrol_Consumption\n",
    "X = dataset[['Petrol_tax','Average_income', 'Population_Driver_licence(%)']]\n",
    "\n",
    "# Adding an intercept *** This is requried ***. Don't forget this step.\n",
    "# The intercept centers the error residuals around zero\n",
    "# which helps to avoid over-fitting.\n",
    "X_withConst = sm.add_constant(X)\n",
    "y = dataset['Petrol_Consumption'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_withConst, y,\n",
    "                                                    test_size=0.2, random_state=0)\n",
    "\n",
    "def performLinearRegression(X_train, X_test, y_train, y_test):\n",
    "    model = sm.OLS(y_train, X_train).fit()\n",
    "    predictions = model.predict(X_test) # make the predictions by the model\n",
    "    print(model.summary())\n",
    "    print('Root Mean Squared Error:',\n",
    "          np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
    "    return predictions\n",
    "\n",
    "predictions = performLinearRegression(X_train, X_test, y_train, y_test)\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "def performLassorRegression(X_train, X_test, y_train, y_test, alpha):\n",
    "    lassoreg = Lasso(alpha=alpha, normalize=True, max_iter=1e5)\n",
    "    lassoreg.fit(X_train, y_train)\n",
    "    y_pred = lassoreg.predict(X_test)\n",
    "    print(\"\\n***Lasso Regression Coefficients ** alpha=\" + str(alpha))\n",
    "    print(lassoreg.intercept_)\n",
    "    print(lassoreg.coef_)\n",
    "    print('Root Mean Squared Error:',\n",
    "          np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "alphaValues = [0, 0.1, 0.5, 1]\n",
    "for i in range(0, len(alphaValues)):\n",
    "    performLassorRegression(X_train, X_test, y_train, y_test,\n",
    "                            alphaValues[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.707\n",
      "Model:                            OLS   Adj. R-squared:                  0.681\n",
      "Method:                 Least Squares   F-statistic:                     27.32\n",
      "Date:                Wed, 09 Feb 2022   Prob (F-statistic):           3.53e-09\n",
      "Time:                        16:40:39   Log-Likelihood:                -211.58\n",
      "No. Observations:                  38   AIC:                             431.2\n",
      "Df Residuals:                      34   BIC:                             437.7\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================================\n",
      "                                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------\n",
      "const                          288.6119    173.511      1.663      0.105     -64.004     641.228\n",
      "Petrol_tax                     -29.3087     11.149     -2.629      0.013     -51.967      -6.651\n",
      "Average_income                  -0.0708      0.020     -3.478      0.001      -0.112      -0.029\n",
      "Population_Driver_licence(%)  1430.5650    199.565      7.168      0.000    1025.001    1836.129\n",
      "==============================================================================\n",
      "Omnibus:                       17.327   Durbin-Watson:                   2.075\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               22.574\n",
      "Skew:                           1.347   Prob(JB):                     1.25e-05\n",
      "Kurtosis:                       5.647   Cond. No.                     9.89e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 9.89e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "Root Mean Squared Error: 62.88023441227515\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0 l1ratio=0\n",
      "288.6119448185816\n",
      "[ 0.00000000e+00 -2.93087031e+01 -7.08395205e-02  1.43056503e+03]\n",
      "Root Mean Squared Error: 62.88023441227642\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0 l1ratio=0.25\n",
      "288.6119448185816\n",
      "[ 0.00000000e+00 -2.93087031e+01 -7.08395205e-02  1.43056503e+03]\n",
      "Root Mean Squared Error: 62.88023441227642\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0 l1ratio=0.5\n",
      "288.6119448185816\n",
      "[ 0.00000000e+00 -2.93087031e+01 -7.08395205e-02  1.43056503e+03]\n",
      "Root Mean Squared Error: 62.88023441227642\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0 l1ratio=0.75\n",
      "288.6119448185816\n",
      "[ 0.00000000e+00 -2.93087031e+01 -7.08395205e-02  1.43056503e+03]\n",
      "Root Mean Squared Error: 62.88023441227642\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0 l1ratio=1\n",
      "288.6119448185816\n",
      "[ 0.00000000e+00 -2.93087031e+01 -7.08395205e-02  1.43056503e+03]\n",
      "Root Mean Squared Error: 62.88023441227642\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=1e-05 l1ratio=0\n",
      "291.69805888556954\n",
      "[ 0.00000000e+00 -2.93757829e+01 -7.07935790e-02  1.42575732e+03]\n",
      "Root Mean Squared Error: 62.83090439937613\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=1e-05 l1ratio=0.25\n",
      "290.9289948950051\n",
      "[ 0.00000000e+00 -2.93590647e+01 -7.08050277e-02  1.42695539e+03]\n",
      "Root Mean Squared Error: 62.843137503589986\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=1e-05 l1ratio=0.5\n",
      "290.15863437332774\n",
      "[ 0.00000000e+00 -2.93423182e+01 -7.08164957e-02  1.42815547e+03]\n",
      "Root Mean Squared Error: 62.85543093330202\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=1e-05 l1ratio=0.75\n",
      "289.386974045909\n",
      "[ 0.00000000e+00 -2.93255432e+01 -7.08279831e-02  1.42935757e+03]\n",
      "Root Mean Squared Error: 62.86778491799275\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=1e-05 l1ratio=1\n",
      "288.6140106238975\n",
      "[ 0.00000000e+00 -2.93087398e+01 -7.08394898e-02  1.43056170e+03]\n",
      "Root Mean Squared Error: 62.880199688056095\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.0001 l1ratio=0\n",
      "318.56561119898504\n",
      "[ 0.00000000e+00 -2.99596562e+01 -7.03936159e-02  1.38390010e+03]\n",
      "Root Mean Squared Error: 62.42856098327666\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.0001 l1ratio=0.25\n",
      "311.26724092951446\n",
      "[ 0.00000000e+00 -2.98010529e+01 -7.05022628e-02  1.39527032e+03]\n",
      "Root Mean Squared Error: 62.53301133014099\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.0001 l1ratio=0.5\n",
      "303.8476607255422\n",
      "[ 0.00000000e+00 -2.96397989e+01 -7.06127142e-02  1.40682916e+03]\n",
      "Root Mean Squared Error: 62.642907572913806\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.0001 l1ratio=0.75\n",
      "296.3038293361205\n",
      "[ 0.00000000e+00 -2.94758277e+01 -7.07250156e-02  1.41858134e+03]\n",
      "Root Mean Squared Error: 62.75845998587695\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.0001 l1ratio=1\n",
      "288.6326028778038\n",
      "[ 0.00000000e+00 -2.93090704e+01 -7.08392137e-02  1.43053177e+03]\n",
      "Root Mean Squared Error: 62.87988718442992\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.001 l1ratio=0\n",
      "520.0644938168746\n",
      "[ 0.00000000e+00 -3.43293166e+01 -6.73941244e-02  1.06986181e+03]\n",
      "Root Mean Squared Error: 61.032080254523905\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.001 l1ratio=0.25\n",
      "473.9440540687412\n",
      "[ 0.00000000e+00 -3.33307498e+01 -6.80806475e-02  1.14176194e+03]\n",
      "Root Mean Squared Error: 61.09281439473488\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.001 l1ratio=0.5\n",
      "421.15454915247176\n",
      "[ 0.00000000e+00 -3.21863400e+01 -6.88664609e-02  1.22403986e+03]\n",
      "Root Mean Squared Error: 61.352947611084886\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.001 l1ratio=0.75\n",
      "360.1397774435401\n",
      "[ 0.00000000e+00 -3.08620539e+01 -6.97747339e-02  1.31911717e+03]\n",
      "Root Mean Squared Error: 61.903422391841985\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.001 l1ratio=1\n",
      "288.8185254826247\n",
      "[ 0.00000000e+00 -2.93123770e+01 -7.08364519e-02  1.43023245e+03]\n",
      "Root Mean Squared Error: 62.87676357216542\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.01 l1ratio=0\n",
      "993.9633417582\n",
      "[ 0.00000000e+00 -4.43268818e+01 -6.03432441e-02  3.27614182e+02]\n",
      "Root Mean Squared Error: 69.06260886151429\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.01 l1ratio=0.25\n",
      "944.7559999966704\n",
      "[ 0.00000000e+00 -4.33497552e+01 -6.10745934e-02  4.05486845e+02]\n",
      "Root Mean Squared Error: 67.54518412180406\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.01 l1ratio=0.5\n",
      "864.1353344667691\n",
      "[ 0.00000000e+00 -4.16861254e+01 -6.22736251e-02  5.32248123e+02]\n",
      "Root Mean Squared Error: 65.38534607939896\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.01 l1ratio=0.75\n",
      "708.9781856341749\n",
      "[ 0.00000000e+00 -3.83929221e+01 -6.45823657e-02  7.75001971e+02]\n",
      "Root Mean Squared Error: 62.40273509420594\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.01 l1ratio=1\n",
      "290.6777523009988\n",
      "[ 0.00000000e+00 -2.93454429e+01 -7.08088338e-02  1.42723925e+03]\n",
      "Root Mean Squared Error: 62.845669956007754\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.18 l1ratio=0\n",
      "1139.5578664014665\n",
      "[  0.         -41.62955158  -0.0582499   23.75438605]\n",
      "Root Mean Squared Error: 77.77808549989903\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.18 l1ratio=0.25\n",
      "1146.3293674142449\n",
      "[  0.         -43.13157437  -0.05813182  31.01108421]\n",
      "Root Mean Squared Error: 77.16635639679015\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.18 l1ratio=0.5\n",
      "1149.5313987061186\n",
      "[  0.         -44.66592657  -0.05806548  45.27312141]\n",
      "Root Mean Squared Error: 76.35696781201597\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.18 l1ratio=0.75\n",
      "1136.6150910429255\n",
      "[ 0.00000000e+00 -4.59722515e+01 -5.82375421e-02  8.62528102e+01]\n",
      "Root Mean Squared Error: 74.86781538526604\n",
      "\n",
      "***ElasticNet Regression Coefficients ** alpha=0.18 l1ratio=1\n",
      "325.79644379395467\n",
      "[ 0.00000000e+00 -2.99700166e+01 -7.02871623e-02  1.37070101e+03]\n",
      "Root Mean Squared Error: 62.30748383575302\n",
      "Best RMSE 61.032080254523905 Best alpha: 0.001  Best l1 ratio: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wl/bvgg9vxx64scj9r0jxzqssj00000gn/T/ipykernel_45815/1918562178.py:38: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  model.fit(X_train, y_train)\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.627e+04, tolerance: 5.203e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/var/folders/wl/bvgg9vxx64scj9r0jxzqssj00000gn/T/ipykernel_45815/1918562178.py:38: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  model.fit(X_train, y_train)\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.627e+04, tolerance: 5.203e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/var/folders/wl/bvgg9vxx64scj9r0jxzqssj00000gn/T/ipykernel_45815/1918562178.py:38: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  model.fit(X_train, y_train)\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.627e+04, tolerance: 5.203e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/var/folders/wl/bvgg9vxx64scj9r0jxzqssj00000gn/T/ipykernel_45815/1918562178.py:38: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  model.fit(X_train, y_train)\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.627e+04, tolerance: 5.203e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/var/folders/wl/bvgg9vxx64scj9r0jxzqssj00000gn/T/ipykernel_45815/1918562178.py:38: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  model.fit(X_train, y_train)\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.627e+04, tolerance: 5.203e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.666e+04, tolerance: 5.203e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.004e+04, tolerance: 5.203e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.054e+05, tolerance: 5.203e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.656e+05, tolerance: 5.203e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:645: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.967e+05, tolerance: 5.203e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn                 import metrics\n",
    "import statsmodels.api       as sm\n",
    "import numpy as np\n",
    "PATH     = \"../datasets/\"\n",
    "CSV_DATA = \"petrol_consumption.csv\"\n",
    "dataset  = pd.read_csv(PATH + CSV_DATA)\n",
    "#   Petrol_Consumption\n",
    "X = dataset[['Petrol_tax','Average_income', 'Population_Driver_licence(%)']]\n",
    "\n",
    "# Adding an intercept *** This is requried ***. Don't forget this step.\n",
    "# The intercept centers the error residuals around zero\n",
    "# which helps to avoid over-fitting.\n",
    "X_withConst = sm.add_constant(X)\n",
    "y = dataset['Petrol_Consumption'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_withConst, y,\n",
    "                                                    test_size=0.2, random_state=0)\n",
    "\n",
    "def performLinearRegression(X_train, X_test, y_train, y_test):\n",
    "    model = sm.OLS(y_train, X_train).fit()\n",
    "    predictions = model.predict(X_test) # make the predictions by the model\n",
    "    print(model.summary())\n",
    "    print('Root Mean Squared Error:',\n",
    "          np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
    "    return predictions\n",
    "\n",
    "predictions = performLinearRegression(X_train, X_test, y_train, y_test)\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "bestRMSE = 100000.03\n",
    "def performElasticNetRegression(X_train, X_test, y_train, y_test, alpha, l1ratio, bestRMSE,\n",
    "                                bestAlpha, bestL1Ratio):\n",
    "    model = ElasticNet(alpha=alpha, l1_ratio=l1ratio)\n",
    "    # fit model\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"\\n***ElasticNet Regression Coefficients ** alpha=\" + str(alpha)\n",
    "          + \" l1ratio=\" + str(l1ratio))\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "    print(model.intercept_)\n",
    "    print(model.coef_)\n",
    "    try:\n",
    "        if(rmse < bestRMSE):\n",
    "            bestRMSE = rmse\n",
    "            bestAlpha = alpha\n",
    "            bestL1Ratio = l1ratio\n",
    "        print('Root Mean Squared Error:', rmse)\n",
    "    except:\n",
    "        print(\"rmse =\" + str(rmse))\n",
    "\n",
    "    return bestRMSE, bestAlpha, bestL1Ratio\n",
    "\n",
    "alphaValues = [0, 0.00001, 0.0001, 0.001, 0.01, 0.18]\n",
    "l1ratioValues = [0, 0.25, 0.5, 0.75, 1]\n",
    "bestAlpha   = 0\n",
    "bestL1Ratio = 0\n",
    "\n",
    "for i in range(0, len(alphaValues)):\n",
    "    for j in range(0, len(l1ratioValues)):\n",
    "        bestRMSE, bestAlpha, bestL1Ratio = performElasticNetRegression(\n",
    "            X_train, X_test, y_train, y_test,\n",
    "            alphaValues[i], l1ratioValues[j], bestRMSE,\n",
    "            bestAlpha, bestL1Ratio)\n",
    "\n",
    "print(\"Best RMSE \" + str(bestRMSE) + \" Best alpha: \" + str(bestAlpha)\n",
    "      + \"  \" + \"Best l1 ratio: \" + str(bestL1Ratio))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lazypredict'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [33]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlazypredict\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mSupervised\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LazyClassifier, LazyRegressor\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m datasets\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'lazypredict'"
     ]
    }
   ],
   "source": [
    "from lazypredict.Supervised import LazyClassifier, LazyRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "# Show all columns on one line.\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# load data\n",
    "data = datasets.load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)# fit all models\n",
    "clf = LazyClassifier(predictions=True)\n",
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "print(models)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}