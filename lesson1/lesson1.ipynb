{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  alcohol  quality\n",
      "0            7.4              0.70         0.00             1.9      0.076                 11.0                  34.0   0.9978  3.51       0.56      9.4        5\n",
      "1            7.8              0.88         0.00             2.6      0.098                 25.0                  67.0   0.9968  3.20       0.68      9.8        5\n",
      "2            7.8              0.76         0.04             2.3      0.092                 15.0                  54.0   0.9970  3.26       0.65      9.8        5\n",
      "3           11.2              0.28         0.56             1.9      0.075                 17.0                  60.0   0.9980  3.16       0.58      9.8        6\n",
      "4            7.4              0.70         0.00             1.9      0.076                 11.0                  34.0   0.9978  3.51       0.56      9.4        5\n",
      "       fixed acidity  volatile acidity  citric acid  residual sugar    chlorides  free sulfur dioxide  total sulfur dioxide      density           pH    sulphates      alcohol      quality\n",
      "count    1599.000000       1599.000000  1599.000000     1599.000000  1599.000000          1599.000000           1599.000000  1599.000000  1599.000000  1599.000000  1599.000000  1599.000000\n",
      "mean        8.319637          0.527821     0.270976        2.538806     0.087467            15.874922             46.467792     0.996747     3.311113     0.658149    10.422983     5.636023\n",
      "std         1.741096          0.179060     0.194801        1.409928     0.047065            10.460157             32.895324     0.001887     0.154386     0.169507     1.065668     0.807569\n",
      "min         4.600000          0.120000     0.000000        0.900000     0.012000             1.000000              6.000000     0.990070     2.740000     0.330000     8.400000     3.000000\n",
      "25%         7.100000          0.390000     0.090000        1.900000     0.070000             7.000000             22.000000     0.995600     3.210000     0.550000     9.500000     5.000000\n",
      "50%         7.900000          0.520000     0.260000        2.200000     0.079000            14.000000             38.000000     0.996750     3.310000     0.620000    10.200000     6.000000\n",
      "75%         9.200000          0.640000     0.420000        2.600000     0.090000            21.000000             62.000000     0.997835     3.400000     0.730000    11.100000     6.000000\n",
      "max        15.900000          1.580000     1.000000       15.500000     0.611000            72.000000            289.000000     1.003690     4.010000     2.000000    14.900000     8.000000\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                quality   R-squared:                       0.358\n",
      "Model:                            OLS   Adj. R-squared:                  0.355\n",
      "Method:                 Least Squares   F-statistic:                     141.8\n",
      "Date:                Tue, 11 Jan 2022   Prob (F-statistic):          1.06e-119\n",
      "Time:                        21:25:53   Log-Likelihood:                -1276.6\n",
      "No. Observations:                1279   AIC:                             2565.\n",
      "Df Residuals:                    1273   BIC:                             2596.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "const                    3.0105      0.226     13.298      0.000       2.566       3.455\n",
      "volatile acidity        -1.2105      0.107    -11.337      0.000      -1.420      -1.001\n",
      "chlorides               -1.7655      0.433     -4.082      0.000      -2.614      -0.917\n",
      "total sulfur dioxide    -0.0022      0.001     -3.859      0.000      -0.003      -0.001\n",
      "sulphates                0.8949      0.120      7.439      0.000       0.659       1.131\n",
      "alcohol                  0.2827      0.019     15.207      0.000       0.246       0.319\n",
      "==============================================================================\n",
      "Omnibus:                       14.372   Durbin-Watson:                   2.100\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               18.599\n",
      "Skew:                          -0.144   Prob(JB):                     9.15e-05\n",
      "Kurtosis:                       3.515   Cond. No.                     1.39e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.39e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "Root Mean Squared Error: 0.6259157889490522\n",
      "\n",
      "Stochastic Gradient Descent\n",
      "-- Epoch 1\n",
      "Norm: 0.43, NNZs: 5, Bias: 5.320559, T: 1279, Avg. loss: 2.144061\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.42, NNZs: 5, Bias: 5.601931, T: 2558, Avg. loss: 0.230404\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.42, NNZs: 5, Bias: 5.632472, T: 3837, Avg. loss: 0.217803\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.42, NNZs: 5, Bias: 5.651449, T: 5116, Avg. loss: 0.217104\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.43, NNZs: 5, Bias: 5.644344, T: 6395, Avg. loss: 0.216810\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.43, NNZs: 5, Bias: 5.641334, T: 7674, Avg. loss: 0.217185\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.44, NNZs: 5, Bias: 5.650779, T: 8953, Avg. loss: 0.216737\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.42, NNZs: 5, Bias: 5.648688, T: 10232, Avg. loss: 0.216817\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 8 epochs took 0.00 seconds\n",
      "Root Mean Squared Error: 0.6259742283910119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/COMP-3948/lib/python3.9/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api       as sm\n",
    "\n",
    "PATH = \"../datasets/\"\n",
    "CSV_DATA = \"winequality.csv\"\n",
    "\n",
    "dataset  = pd.read_csv(PATH + CSV_DATA,\n",
    "                       skiprows=1,       # Don't include header row as part of data.\n",
    "                       encoding = \"ISO-8859-1\", sep=',',\n",
    "                       names=('fixed acidity', 'volatile acidity', 'citric acid',\n",
    "                              'residual sugar', 'chlorides', 'free sulfur dioxide',\n",
    "                              'total sulfur dioxide', 'density', 'pH', 'sulphates',\n",
    "                              'alcohol', 'quality'))\n",
    "# Show all columns.\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Increase number of columns that display on one line.\n",
    "pd.set_option('display.width', 1000)\n",
    "print(dataset.head())\n",
    "print(dataset.describe())\n",
    "X = dataset[['volatile acidity',\n",
    "             'chlorides', 'total sulfur dioxide', 'sulphates','alcohol']]\n",
    "\n",
    "# Adding an intercept *** This is required ***. Don't forget this step.\n",
    "# The intercept centers the error residuals around zero\n",
    "# which helps to avoid over-fitting.\n",
    "X = sm.add_constant(X)\n",
    "y = dataset['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "predictions = model.predict(X_test) # make the predictions by the model\n",
    "print(model.summary())\n",
    "print('Root Mean Squared Error:',\n",
    "      np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
    "\n",
    "###########################################################\n",
    "print(\"\\nStochastic Gradient Descent\")\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Stochastic gradient descent models are sensitive to differences\n",
    "# in scale so a StandardScaler is usually used.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  # Don't cheat - fit only on training data\n",
    "X_trainScaled = scaler.transform(X_train)\n",
    "X_testScaled  = scaler.transform(X_test)\n",
    "\n",
    "# SkLearn SGD classifier\n",
    "sgd = SGDRegressor(verbose=1)\n",
    "sgd.fit(X_trainScaled, y_train)\n",
    "predictions = sgd.predict(X_testScaled)\n",
    "print('Root Mean Squared Error:',\n",
    "      np.sqrt(mean_squared_error(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    gmat  gpa  work_experience  admitted\n",
      "0    780  4.0                3         1\n",
      "1    750  3.9                4         1\n",
      "2    690  3.3                3         1\n",
      "3    710  3.7                5         1\n",
      "4    680  3.9                4         1\n",
      "5    730  3.7                6         1\n",
      "6    690  2.3                1         0\n",
      "7    720  3.3                4         1\n",
      "8    740  3.3                5         1\n",
      "9    690  1.7                1         0\n",
      "10   610  2.7                3         0\n",
      "11   690  3.7                5         1\n",
      "12   710  3.7                6         1\n",
      "13   680  3.3                4         1\n",
      "14   770  3.3                3         1\n",
      "15   610  3.0                1         0\n",
      "16   580  2.7                4         0\n",
      "17   650  3.7                6         1\n",
      "18   540  2.7                2         0\n",
      "19   590  2.3                3         0\n",
      "20   620  3.3                2         0\n",
      "21   600  2.0                1         0\n",
      "22   550  2.3                4         0\n",
      "23   550  2.7                1         0\n",
      "24   570  3.0                2         0\n",
      "25   670  3.3                6         1\n",
      "26   660  3.7                4         1\n",
      "27   580  2.3                2         0\n",
      "28   650  3.7                6         1\n",
      "29   660  3.3                5         1\n",
      "30   640  3.0                1         0\n",
      "31   620  2.7                2         0\n",
      "32   660  4.0                4         1\n",
      "33   660  3.3                6         1\n",
      "34   680  3.3                5         1\n",
      "35   650  2.3                1         0\n",
      "36   670  2.7                2         0\n",
      "37   580  3.3                1         0\n",
      "38   590  1.7                4         0\n",
      "39   690  3.7                5         1\n",
      "\n",
      "Predictor Chi-Square Scores: [123.062   3.307  21.457]\n",
      "\n",
      "Model Coefficients: \n",
      "\n",
      "Intercept: \n",
      "[0.176]\n",
      "[[1.546 1.711]]\n",
      "\n",
      "Accuracy:  1.0\n",
      "\n",
      "Confusion Matrix\n",
      "Predicted  0  1\n",
      "Actual         \n",
      "0          5  0\n",
      "1          0  5\n",
      "\n",
      "Stochastic Gradient Descent\n",
      "\n",
      "Accuracy:  1.0\n",
      "\n",
      "Confusion Matrix\n",
      "Predicted  0  1\n",
      "Actual         \n",
      "0          5  0\n",
      "1          0  5\n"
     ]
    }
   ],
   "source": [
    "import pandas  as pd\n",
    "import numpy   as np\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   sklearn.linear_model    import LogisticRegression\n",
    "from   sklearn                 import metrics\n",
    "\n",
    "# Setup data.\n",
    "candidates = {'gmat': [780,750,690,710,680,730,690,720,\n",
    "                       740,690,610,690,710,680,770,610,580,650,540,590,620,\n",
    "                       600,550,550,570,670,660,580,650,660,640,620,660,660,\n",
    "                       680,650,670,580,590,690],\n",
    "              'gpa': [4,3.9,3.3,3.7,3.9,3.7,2.3,3.3,\n",
    "                      3.3,1.7,2.7,3.7,3.7,3.3,3.3,3,2.7,3.7,2.7,2.3,\n",
    "                      3.3,2,2.3,2.7,3,3.3,3.7,2.3,3.7,3.3,3,2.7,4,\n",
    "                      3.3,3.3,2.3,2.7,3.3,1.7,3.7],\n",
    "              'work_experience': [3,4,3,5,4,6,1,4,5,\n",
    "                                  1,3,5,6,4,3,1,4,6,2,3,2,1,4,1,2,6,4,2,6,5,1,2,4,6,\n",
    "                                  5,1,2,1,4,5],\n",
    "              'admitted': [1,1,1,1,1,1,0,1,1,0,0,1,\n",
    "                           1,1,1,0,0,1,0,0,0,0,0,0,0,1,1,0,1,1,0,0,1,1,1,0,0,\n",
    "                           0,0,1]}\n",
    "\n",
    "df = pd.DataFrame(candidates,columns= ['gmat', 'gpa',\n",
    "                                       'work_experience','admitted'])\n",
    "print(df)\n",
    "\n",
    "# Separate into x and y values.\n",
    "X = df[['gmat', 'gpa','work_experience']]\n",
    "y = df['admitted']\n",
    "\n",
    "# Import the necessary libraries first\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Show chi-square scores for each feature.\n",
    "# There is 1-degree freedom since 1 predictor during feature evaluation.\n",
    "# Generally, >=3.8 is good)\n",
    "test      = SelectKBest(score_func=chi2, k=3)\n",
    "chiScores = test.fit(X, y) # Summarize scores\n",
    "np.set_printoptions(precision=3)\n",
    "print(\"\\nPredictor Chi-Square Scores: \" + str(chiScores.scores_))\n",
    "\n",
    "# Re-assign X with significant columns only after chi-square test.\n",
    "X = df[['gmat', 'work_experience']]\n",
    "\n",
    "# Split data.\n",
    "X_train,X_test,y_train,y_test = train_test_split(\n",
    "    X, y, test_size=0.25,random_state=0)\n",
    "\n",
    "# Perform logistic regression.\n",
    "logisticModel = LogisticRegression(fit_intercept=True, random_state = 0,\n",
    "                                   solver='liblinear')\n",
    "\n",
    "# Stochastic gradient descent models are sensitive to differences\n",
    "# in scale so a StandardScaler is usually used.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_trainScaled = scaler.transform(X_train)\n",
    "X_testScaled  = scaler.transform(X_test)\n",
    "\n",
    "logisticModel.fit(X_trainScaled,y_train)\n",
    "y_pred=logisticModel.predict(X_testScaled)\n",
    "\n",
    "# Show model coefficients and intercept.\n",
    "print(\"\\nModel Coefficients: \")\n",
    "print(\"\\nIntercept: \")\n",
    "print(logisticModel.intercept_)\n",
    "\n",
    "print(logisticModel.coef_)\n",
    "\n",
    "# Show confusion matrix and accuracy scores.\n",
    "confusion_matrix = pd.crosstab(y_test, y_pred,\n",
    "                               rownames=['Actual'],\n",
    "                               colnames=['Predicted'])\n",
    "\n",
    "print('\\nAccuracy: ',metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix)\n",
    "\n",
    "print(\"\\nStochastic Gradient Descent\")\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X_trainScaled, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_testScaled)\n",
    "\n",
    "# Show confusion matrix and accuracy scores.\n",
    "confusion_matrix = pd.crosstab(y_test, y_pred,\n",
    "                               rownames=['Actual'],\n",
    "                               colnames=['Predicted'])\n",
    "\n",
    "print('\\nAccuracy: ',metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 0.95 Res: -0.0\n"
     ]
    }
   ],
   "source": [
    "weights = [0.5, 2.3, 2.9]\n",
    "heights = [1.4, 1.9, 3.2]\n",
    "\n",
    "def getRes(weights, heights, intercept):\n",
    "    sum  = 0\n",
    "    BETA = 0.64\n",
    "    for i in range(0, len(weights)):\n",
    "        sum+= -2*(heights[i] - intercept - BETA*weights[i])\n",
    "\n",
    "    print(\"Intercept: \" + str(intercept) + \" Res: \" + str(round(sum,2)) )\n",
    "\n",
    "intercept = 0.95\n",
    "getRes(weights, heights, intercept)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}