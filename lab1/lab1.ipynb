{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  alcohol  quality\n",
      "0            7.4              0.70         0.00             1.9      0.076                 11.0                  34.0   0.9978  3.51       0.56      9.4        5\n",
      "1            7.8              0.88         0.00             2.6      0.098                 25.0                  67.0   0.9968  3.20       0.68      9.8        5\n",
      "2            7.8              0.76         0.04             2.3      0.092                 15.0                  54.0   0.9970  3.26       0.65      9.8        5\n",
      "3           11.2              0.28         0.56             1.9      0.075                 17.0                  60.0   0.9980  3.16       0.58      9.8        6\n",
      "4            7.4              0.70         0.00             1.9      0.076                 11.0                  34.0   0.9978  3.51       0.56      9.4        5\n",
      "       fixed acidity  volatile acidity  citric acid  residual sugar    chlorides  free sulfur dioxide  total sulfur dioxide      density           pH    sulphates      alcohol      quality\n",
      "count    1599.000000       1599.000000  1599.000000     1599.000000  1599.000000          1599.000000           1599.000000  1599.000000  1599.000000  1599.000000  1599.000000  1599.000000\n",
      "mean        8.319637          0.527821     0.270976        2.538806     0.087467            15.874922             46.467792     0.996747     3.311113     0.658149    10.422983     5.636023\n",
      "std         1.741096          0.179060     0.194801        1.409928     0.047065            10.460157             32.895324     0.001887     0.154386     0.169507     1.065668     0.807569\n",
      "min         4.600000          0.120000     0.000000        0.900000     0.012000             1.000000              6.000000     0.990070     2.740000     0.330000     8.400000     3.000000\n",
      "25%         7.100000          0.390000     0.090000        1.900000     0.070000             7.000000             22.000000     0.995600     3.210000     0.550000     9.500000     5.000000\n",
      "50%         7.900000          0.520000     0.260000        2.200000     0.079000            14.000000             38.000000     0.996750     3.310000     0.620000    10.200000     6.000000\n",
      "75%         9.200000          0.640000     0.420000        2.600000     0.090000            21.000000             62.000000     0.997835     3.400000     0.730000    11.100000     6.000000\n",
      "max        15.900000          1.580000     1.000000       15.500000     0.611000            72.000000            289.000000     1.003690     4.010000     2.000000    14.900000     8.000000\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.362\n",
      "Model:                            OLS   Adj. R-squared:                  0.359\n",
      "Method:                 Least Squares   F-statistic:                     120.4\n",
      "Date:                Tue, 11 Jan 2022   Prob (F-statistic):          1.56e-120\n",
      "Time:                        23:32:46   Log-Likelihood:                 786.40\n",
      "No. Observations:                1279   AIC:                            -1559.\n",
      "Df Residuals:                    1272   BIC:                            -1523.\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.5454      0.021     26.484      0.000       0.505       0.586\n",
      "x1            -0.3252      0.032    -10.013      0.000      -0.389      -0.261\n",
      "x2            -0.2420      0.053     -4.597      0.000      -0.345      -0.139\n",
      "x3            -0.1268      0.032     -3.985      0.000      -0.189      -0.064\n",
      "x4            -0.1004      0.034     -2.995      0.003      -0.166      -0.035\n",
      "x5             0.2839      0.039      7.245      0.000       0.207       0.361\n",
      "x6             0.3831      0.025     15.544      0.000       0.335       0.431\n",
      "==============================================================================\n",
      "Omnibus:                       14.694   Durbin-Watson:                   2.089\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               18.809\n",
      "Skew:                          -0.152   Prob(JB):                     8.23e-05\n",
      "Kurtosis:                       3.511   Cond. No.                         19.0\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "Root Mean Squared Error: 0.6206983401233145\n",
      "\n",
      "Ridge Regression\n",
      "Root Mean Squared Error: 0.6199800116556096\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api       as sm\n",
    "\n",
    "PATH     = \"../datasets/\"\n",
    "CSV_DATA = \"winequality.csv\"\n",
    "\n",
    "dataset  = pd.read_csv(PATH + CSV_DATA,\n",
    "                       skiprows=1,       # Don't include header row as part of data.\n",
    "                       encoding = \"ISO-8859-1\", sep=',',\n",
    "                       names=('fixed acidity', 'volatile acidity', 'citric acid',\n",
    "                              'residual sugar', 'chlorides', 'free sulfur dioxide',\n",
    "                              'total sulfur dioxide', 'density', 'pH', 'sulphates',\n",
    "                              'alcohol', 'quality'))\n",
    "# Show all columns.\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Increase number of columns that display on one line.\n",
    "pd.set_option('display.width', 1000)\n",
    "print(dataset.head())\n",
    "print(dataset.describe())\n",
    "\n",
    "# Include only statistically significant columns.\n",
    "X = dataset[['volatile acidity',\n",
    "             'chlorides', 'total sulfur dioxide',\n",
    "             'pH', 'sulphates','alcohol']]\n",
    "y = dataset['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Stochastic gradient descent models are sensitive to differences\n",
    "# in scale so a MinMax is usually used.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scalerX = MinMaxScaler()\n",
    "scalerX.fit(X_train)\n",
    "\n",
    "# Build scaler for y.\n",
    "scalerY = MinMaxScaler()\n",
    "reshapedYtrain = np.array(y_train).reshape(-1,1)\n",
    "scalerY.fit(reshapedYtrain)\n",
    "\n",
    "# Scale X_train, X_test and y_train.\n",
    "X_trainScaled = scalerX.transform(X_train)\n",
    "X_testScaled  = scalerX.transform(X_test)\n",
    "y_trainScaled = scalerY.transform(reshapedYtrain)\n",
    "\n",
    "# Add constant to scaled data.\n",
    "X_trainScaled = sm.add_constant(X_trainScaled)\n",
    "X_testScaled  = sm.add_constant(X_testScaled)\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "# Perform OLS regression.\n",
    "model       = sm.OLS(y_trainScaled, X_trainScaled).fit()\n",
    "predictions = model.predict(X_testScaled) # make the predictions by the model\n",
    "print(model.summary())\n",
    "\n",
    "# Convert predictions to unscaled predictions and compare with y_test.\n",
    "unscaledPredictionsOLS = scalerY.inverse_transform(predictions.reshape(-1,1))\n",
    "print('Root Mean Squared Error:',\n",
    "      np.sqrt(mean_squared_error(y_test, unscaledPredictionsOLS)))\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "# Perform Ridge regression.\n",
    "print(\"\\nRidge Regression\")\n",
    "from sklearn.linear_model import  Ridge\n",
    "ridge_reg   = Ridge(solver='auto')\n",
    "ridge_reg.fit(X_trainScaled, y_trainScaled)\n",
    "predictions = ridge_reg.predict(X_testScaled)\n",
    "\n",
    "# Convert predictions to unscaled predictions and compare with y_test.\n",
    "unscaledPredictionsRidge = scalerY.inverse_transform(predictions.reshape(-1,1))\n",
    "print('Root Mean Squared Error:',\n",
    "      np.sqrt(mean_squared_error(y_test, unscaledPredictionsRidge)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Avg. Area Income  Avg. Area House Age  Avg. Area Number of Rooms  Avg. Area Number of Bedrooms  Area Population         Price                                            Address\n",
      "0      79545.458574             5.682861                   7.009188                          4.09     23086.800503  1.059034e+06  208 Michael Ferry Apt. 674\\nLaurabury, NE 3701...\n",
      "1      79248.642455             6.002900                   6.730821                          3.09     40173.072174  1.505891e+06  188 Johnson Views Suite 079\\nLake Kathleen, CA...\n",
      "2      61287.067179             5.865890                   8.512727                          5.13     36882.159400  1.058988e+06  9127 Elizabeth Stravenue\\nDanieltown, WI 06482...\n",
      "3      63345.240046             7.188236                   5.586729                          3.26     34310.242831  1.260617e+06                          USS Barnett\\nFPO AP 44820\n",
      "4      59982.197226             5.040555                   7.839388                          4.23     26354.109472  6.309435e+05                         USNS Raymond\\nFPO AE 09386\n",
      "       Avg. Area Income  Avg. Area House Age  Avg. Area Number of Rooms  Avg. Area Number of Bedrooms  Area Population         Price\n",
      "count       5000.000000          5000.000000                5000.000000                   5000.000000      5000.000000  5.000000e+03\n",
      "mean       68583.108984             5.977222                   6.987792                      3.981330     36163.516039  1.232073e+06\n",
      "std        10657.991214             0.991456                   1.005833                      1.234137      9925.650114  3.531176e+05\n",
      "min        17796.631190             2.644304                   3.236194                      2.000000       172.610686  1.593866e+04\n",
      "25%        61480.562388             5.322283                   6.299250                      3.140000     29403.928702  9.975771e+05\n",
      "50%        68804.286404             5.970429                   7.002902                      4.050000     36199.406689  1.232669e+06\n",
      "75%        75783.338666             6.650808                   7.665871                      4.490000     42861.290769  1.471210e+06\n",
      "max       107701.748378             9.519088                  10.759588                      6.500000     69621.713378  2.469066e+06\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.919\n",
      "Model:                            OLS   Adj. R-squared:                  0.919\n",
      "Method:                 Least Squares   F-statistic:                 1.130e+04\n",
      "Date:                Tue, 11 Jan 2022   Prob (F-statistic):               0.00\n",
      "Time:                        23:41:14   Log-Likelihood:                 7095.2\n",
      "No. Observations:                4000   AIC:                        -1.418e+04\n",
      "Df Residuals:                    3995   BIC:                        -1.415e+04\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.3736      0.004    -87.658      0.000      -0.382      -0.365\n",
      "x1          3.173e-16   2.22e-18    142.928      0.000    3.13e-16    3.22e-16\n",
      "x2             0.6381      0.004    145.060      0.000       0.630       0.647\n",
      "x3             0.4647      0.004    103.719      0.000       0.456       0.474\n",
      "x4             0.3374      0.004     77.015      0.000       0.329       0.346\n",
      "x5             0.4094      0.004     94.821      0.000       0.401       0.418\n",
      "==============================================================================\n",
      "Omnibus:                        4.614   Durbin-Watson:                   2.015\n",
      "Prob(Omnibus):                  0.100   Jarque-Bera (JB):                4.261\n",
      "Skew:                          -0.035   Prob(JB):                        0.119\n",
      "Kurtosis:                       2.856   Cond. No.                     5.16e+17\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 2.85e-32. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "Root Mean Squared Error: 102671.05426154629\n",
      "\n",
      "Ridge Regression\n",
      "Root Mean Squared Error: 102552.70189365737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/COMP-3948/lib/python3.9/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api       as sm\n",
    "\n",
    "PATH = \"../datasets/\"\n",
    "CSV_DATA = \"USA_Housing.csv\"\n",
    "\n",
    "df  = pd.read_csv(PATH + CSV_DATA)\n",
    "# Show all columns.\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Increase number of columns that display on one line.\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(df.head())\n",
    "print(df.describe())\n",
    "X = df[['Avg. Area Income', 'Avg. Area House Age',  'Avg. Area Number of Rooms', 'Area Population']]\n",
    "\n",
    "# Adding an intercept *** This is required ***. Don't forget this step.\n",
    "# The intercept centers the error residuals around zero\n",
    "# which helps to avoid over-fitting.\n",
    "X = sm.add_constant(X)\n",
    "y = df['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scalerX = MinMaxScaler()\n",
    "scalerX.fit(X_train)\n",
    "\n",
    "# Build scaler for y.\n",
    "scalerY = MinMaxScaler()\n",
    "reshapedYtrain = np.array(y_train).reshape(-1,1)\n",
    "scalerY.fit(reshapedYtrain)\n",
    "\n",
    "# Scale X_train, X_test and y_train.\n",
    "X_trainScaled = scalerX.transform(X_train)\n",
    "X_testScaled  = scalerX.transform(X_test)\n",
    "y_trainScaled = scalerY.transform(reshapedYtrain)\n",
    "\n",
    "# Add constant to scaled data.\n",
    "X_trainScaled = sm.add_constant(X_trainScaled)\n",
    "X_testScaled  = sm.add_constant(X_testScaled)\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "# Perform OLS regression.\n",
    "model       = sm.OLS(y_trainScaled, X_trainScaled).fit()\n",
    "predictions = model.predict(X_testScaled) # make the predictions by the model\n",
    "print(model.summary())\n",
    "\n",
    "# Convert predictions to unscaled predictions and compare with y_test.\n",
    "unscaledPredictionsOLS = scalerY.inverse_transform(predictions.reshape(-1,1))\n",
    "print('Root Mean Squared Error:',\n",
    "      np.sqrt(mean_squared_error(y_test, unscaledPredictionsOLS)))\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "# Perform Ridge regression.\n",
    "print(\"\\nRidge Regression\")\n",
    "from sklearn.linear_model import  Ridge\n",
    "ridge_reg   = Ridge(solver='auto')\n",
    "ridge_reg.fit(X_trainScaled, y_trainScaled)\n",
    "predictions = ridge_reg.predict(X_testScaled)\n",
    "\n",
    "# Convert predictions to unscaled predictions and compare with y_test.\n",
    "unscaledPredictionsRidge = scalerY.inverse_transform(predictions.reshape(-1,1))\n",
    "print('Root Mean Squared Error:',\n",
    "      np.sqrt(mean_squared_error(y_test, unscaledPredictionsRidge)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    gmat  gpa  work_experience  admitted\n",
      "0    780  4.0                3         1\n",
      "1    750  3.9                4         1\n",
      "2    690  3.3                3         1\n",
      "3    710  3.7                5         1\n",
      "4    680  3.9                4         1\n",
      "5    730  3.7                6         1\n",
      "6    690  2.3                1         0\n",
      "7    720  3.3                4         1\n",
      "8    740  3.3                5         1\n",
      "9    690  1.7                1         0\n",
      "10   610  2.7                3         0\n",
      "11   690  3.7                5         1\n",
      "12   710  3.7                6         1\n",
      "13   680  3.3                4         1\n",
      "14   770  3.3                3         1\n",
      "15   610  3.0                1         0\n",
      "16   580  2.7                4         0\n",
      "17   650  3.7                6         1\n",
      "18   540  2.7                2         0\n",
      "19   590  2.3                3         0\n",
      "20   620  3.3                2         0\n",
      "21   600  2.0                1         0\n",
      "22   550  2.3                4         0\n",
      "23   550  2.7                1         0\n",
      "24   570  3.0                2         0\n",
      "25   670  3.3                6         1\n",
      "26   660  3.7                4         1\n",
      "27   580  2.3                2         0\n",
      "28   650  3.7                6         1\n",
      "29   660  3.3                5         1\n",
      "30   640  3.0                1         0\n",
      "31   620  2.7                2         0\n",
      "32   660  4.0                4         1\n",
      "33   660  3.3                6         1\n",
      "34   680  3.3                5         1\n",
      "35   650  2.3                1         0\n",
      "36   670  2.7                2         0\n",
      "37   580  3.3                1         0\n",
      "38   590  1.7                4         0\n",
      "39   690  3.7                5         1\n",
      "\n",
      "Predictor Chi-Square Scores: [123.062   3.307  21.457]\n",
      "\n",
      "Ridge Classifier\n",
      "[0 0 1 1 0 0 1 1 0 1]\n",
      "\n",
      "Accuracy:  1.0\n",
      "\n",
      "Confusion Matrix\n",
      "Predicted  0  1\n",
      "Actual         \n",
      "0          5  0\n",
      "1          0  5\n"
     ]
    }
   ],
   "source": [
    "import pandas  as pd\n",
    "import numpy   as np\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   sklearn.linear_model    import LogisticRegression\n",
    "from   sklearn                 import metrics\n",
    "\n",
    "# Setup data.\n",
    "candidates = {'gmat': [780,750,690,710,680,730,690,720,\n",
    "                       740,690,610,690,710,680,770,610,580,650,540,590,620,\n",
    "                       600,550,550,570,670,660,580,650,660,640,620,660,660,\n",
    "                       680,650,670,580,590,690],\n",
    "              'gpa': [4,3.9,3.3,3.7,3.9,3.7,2.3,3.3,\n",
    "                      3.3,1.7,2.7,3.7,3.7,3.3,3.3,3,2.7,3.7,2.7,2.3,\n",
    "                      3.3,2,2.3,2.7,3,3.3,3.7,2.3,3.7,3.3,3,2.7,4,\n",
    "                      3.3,3.3,2.3,2.7,3.3,1.7,3.7],\n",
    "              'work_experience': [3,4,3,5,4,6,1,4,5,\n",
    "                                  1,3,5,6,4,3,1,4,6,2,3,2,1,4,1,2,6,4,2,6,5,1,2,4,6,\n",
    "                                  5,1,2,1,4,5],\n",
    "              'admitted': [1,1,1,1,1,1,0,1,1,0,0,1,\n",
    "                           1,1,1,0,0,1,0,0,0,0,0,0,0,1,1,0,1,1,0,0,1,1,1,0,0,\n",
    "                           0,0,1]}\n",
    "\n",
    "df = pd.DataFrame(candidates,columns= ['gmat', 'gpa',\n",
    "                                       'work_experience','admitted'])\n",
    "print(df)\n",
    "\n",
    "# Separate into x and y values.\n",
    "X = df[['gmat', 'gpa','work_experience']]\n",
    "y = df['admitted']\n",
    "\n",
    "# Import the necessary libraries first\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Show chi-square scores for each feature.\n",
    "# There is 1-degree freedom since 1 predictor during feature evaluation.\n",
    "# Generally, >=3.8 is good)\n",
    "test      = SelectKBest(score_func=chi2, k=3)\n",
    "chiScores = test.fit(X, y) # Summarize scores\n",
    "np.set_printoptions(precision=3)\n",
    "print(\"\\nPredictor Chi-Square Scores: \" + str(chiScores.scores_))\n",
    "\n",
    "# Re-assign X with significant columns only after chi-square test.\n",
    "X = df[['gmat', 'work_experience']]\n",
    "\n",
    "# Split data.\n",
    "X_train,X_test,y_train,y_test = train_test_split(\n",
    "    X, y, test_size=0.25,random_state=0)\n",
    "\n",
    "# Perform logistic regression.\n",
    "logisticModel = LogisticRegression(fit_intercept=True, random_state = 0,\n",
    "                                   solver='liblinear')\n",
    "\n",
    "# Stochastic gradient descent models are sensitive to differences\n",
    "# in scale so a StandardScaler is usually used.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_trainScaled = scaler.transform(X_train)\n",
    "X_testScaled  = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nRidge Classifier\")\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "clf = RidgeClassifier(solver='auto')\n",
    "clf.fit(X_trainScaled, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_testScaled)\n",
    "print(y_pred)\n",
    "\n",
    "# Show confusion matrix and accuracy scores.\n",
    "confusion_matrix = pd.crosstab(y_test, y_pred,\n",
    "                               rownames=['Actual'],\n",
    "                               colnames=['Predicted'])\n",
    "\n",
    "print('\\nAccuracy: ',metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  ca  thal  target\n",
      "0     63    1   3       145   233    1        0      150      0      2.3      0   0     1       1\n",
      "1     37    1   2       130   250    0        1      187      0      3.5      0   0     2       1\n",
      "2     41    0   1       130   204    0        0      172      0      1.4      2   0     2       1\n",
      "3     56    1   1       120   236    0        1      178      0      0.8      2   0     2       1\n",
      "4     57    0   0       120   354    0        1      163      1      0.6      2   0     2       1\n",
      "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...    ...  ..   ...     ...\n",
      "293   57    0   0       140   241    0        1      123      1      0.2      1   0     3       0\n",
      "294   45    1   3       110   264    0        1      132      0      1.2      1   0     3       0\n",
      "295   68    1   0       144   193    1        1      141      0      3.4      1   2     3       0\n",
      "296   57    1   0       130   131    0        1      115      1      1.2      1   1     3       0\n",
      "297   57    1   0       130   120    0        1      125      0      0.0      1   1     2       0\n",
      "\n",
      "[298 rows x 14 columns]\n",
      "\n",
      "Predictor Chi-Square Scores: [2.406e+01 7.587e+00 6.191e+01 1.420e+01 1.924e+01 1.044e-01 2.553e+00\n",
      " 1.908e+02 3.759e+01 7.196e+01 9.496e+00 6.139e+01 5.773e+00]\n",
      "\n",
      "Logistic  Regression\n",
      "\n",
      "Accuracy:  0.8666666666666667\n",
      "\n",
      "Confusion Matrix\n",
      "Predicted   0   1\n",
      "Actual           \n",
      "0          34   7\n",
      "1           3  31\n",
      "\n",
      "Ridge Classifier\n",
      "\n",
      "Accuracy:  0.8533333333333334\n",
      "\n",
      "Confusion Matrix\n",
      "Predicted   0   1\n",
      "Actual           \n",
      "0          33   8\n",
      "1           3  31\n",
      "----- Using Pickle File -----\n",
      "[1 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1\n",
      " 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 0 1\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "import pandas  as pd\n",
    "import numpy   as np\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   sklearn.linear_model    import LogisticRegression\n",
    "from   sklearn                 import metrics\n",
    "import statsmodels.api        as sm\n",
    "\n",
    "PATH = \"../datasets/\"\n",
    "FILE  = 'heart_disease.csv'\n",
    "df = pd.read_csv(PATH + FILE)\n",
    "print(df)\n",
    "\n",
    "# Separate into x and y values.\n",
    "X = df[['age',\n",
    "        'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang',\n",
    "        'oldpeak', 'slope', 'ca', 'thal']]\n",
    "y = df['target']\n",
    "\n",
    "# Import the necessary libraries first\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Show chi-square scores for each feature.\n",
    "# There is 1-degree freedom since 1 predictor during feature evaluation.\n",
    "# Generally, >=3.8 is good)\n",
    "test      = SelectKBest(score_func=chi2, k=3)\n",
    "chiScores = test.fit(X, y) # Summarize scores\n",
    "np.set_printoptions(precision=3)\n",
    "print(\"\\nPredictor Chi-Square Scores: \" + str(chiScores.scores_))\n",
    "\n",
    "# Re-assign X with significant columns only after chi-square test.\n",
    "X = df[['age',\n",
    "        'sex', 'cp', 'trestbps', 'chol', 'fbs',  'thalach', 'exang',\n",
    "        'oldpeak', 'slope', 'ca', 'thal']]\n",
    "\n",
    "# Split data.\n",
    "X_train,X_test,y_train,y_test = train_test_split(\n",
    "    X, y, test_size=0.25,random_state=0)\n",
    "\n",
    "# Stochastic gradient descent models are sensitive to differences\n",
    "# in scale so a StandardScaler is usually used.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_trainScaled = scaler.transform(X_train)\n",
    "X_testScaled  = scaler.transform(X_test)\n",
    "X_trainScaled = sm.add_constant(X_trainScaled)\n",
    "X_testScaled  = sm.add_constant(X_testScaled)\n",
    "\n",
    "print(\"\\nLogistic  Regression\")\n",
    "# Perform logistic regression.\n",
    "logisticModel = LogisticRegression(fit_intercept=True, random_state = 0,\n",
    "                                   solver='liblinear')\n",
    "\n",
    "logisticModel.fit(X_trainScaled, y_train)\n",
    "y_pred = logisticModel.predict(X_testScaled)\n",
    "\n",
    "# Show confusion matrix and accuracy scores.\n",
    "confusion_matrix = pd.crosstab(y_test, y_pred,\n",
    "                               rownames=['Actual'],\n",
    "                               colnames=['Predicted'])\n",
    "print('\\nAccuracy: ', metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix)\n",
    "\n",
    "print(\"\\nRidge Classifier\")\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "clf = RidgeClassifier(solver='auto')\n",
    "clf.fit(X_trainScaled, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_testScaled)\n",
    "\n",
    "# Show confusion matrix and accuracy scores.\n",
    "confusion_matrix = pd.crosstab(y_test, y_pred,\n",
    "                               rownames=['Actual'],\n",
    "                               colnames=['Predicted'])\n",
    "\n",
    "print('\\nAccuracy: ', metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(confusion_matrix)\n",
    "\n",
    "# Pickling\n",
    "import pickle\n",
    "# save model to file\n",
    "pickle.dump(clf, open(\"myRidgeModel.dat\", \"wb\"))\n",
    "\n",
    "# load model from file\n",
    "loaded_model = pickle.load(open(\"myRidgeModel.dat\", \"rb\"))\n",
    "print(\"----- Using Pickle File -----\")\n",
    "ridgePredictions = loaded_model.predict(X_testScaled)\n",
    "print(ridgePredictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}