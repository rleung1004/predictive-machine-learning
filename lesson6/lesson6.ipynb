{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     year  month  day   week  temp_2  temp_1  average  actual  forecast_noaa  forecast_acc  forecast_under  friend\n",
      "0    2016      1    1    Fri      45      45     45.6      45             43            50              44      29\n",
      "1    2016      1    2    Sat      44      45     45.7      44             41            50              44      61\n",
      "2    2016      1    3    Sun      45      44     45.8      41             43            46              47      56\n",
      "3    2016      1    4    Mon      44      41     45.9      40             44            48              46      53\n",
      "4    2016      1    5   Tues      41      40     46.0      44             46            46              46      41\n",
      "..    ...    ...  ...    ...     ...     ...      ...     ...            ...           ...             ...     ...\n",
      "343  2016     12   27   Tues      42      42     45.2      47             41            50              47      47\n",
      "344  2016     12   28    Wed      42      47     45.3      48             41            49              44      58\n",
      "345  2016     12   29  Thurs      47      48     45.3      48             43            50              45      65\n",
      "346  2016     12   30    Fri      48      48     45.4      57             44            46              44      42\n",
      "347  2016     12   31    Sat      48      57     45.5      40             42            48              47      57\n",
      "\n",
      "[348 rows x 12 columns]\n",
      "   year  month  day  temp_2  temp_1  average  actual  forecast_noaa  forecast_acc  forecast_under  friend  week_Fri  week_Mon  week_Sat  week_Sun  week_Thurs  week_Tues  week_Wed\n",
      "0  2016      1    1      45      45     45.6      45             43            50              44      29         1         0         0         0           0          0         0\n",
      "1  2016      1    2      44      45     45.7      44             41            50              44      61         0         0         1         0           0          0         0\n",
      "2  2016      1    3      45      44     45.8      41             43            46              47      56         0         0         0         1           0          0         0\n",
      "3  2016      1    4      44      41     45.9      40             44            48              46      53         0         1         0         0           0          0         0\n",
      "4  2016      1    5      41      40     46.0      44             46            46              46      41         0         0         0         0           0          1         0\n",
      "Mean Absolute Error: 3.87 degrees.\n",
      "Accuracy: 93.93 %.\n",
      "RMSE: 5.101657512937373\n",
      "Variable: temp_1               Importance: 0.66\n",
      "Variable: average              Importance: 0.15\n",
      "Variable: forecast_noaa        Importance: 0.05\n",
      "Variable: forecast_acc         Importance: 0.03\n",
      "Variable: day                  Importance: 0.02\n",
      "Variable: temp_2               Importance: 0.02\n",
      "Variable: forecast_under       Importance: 0.02\n",
      "Variable: friend               Importance: 0.02\n",
      "Variable: month                Importance: 0.01\n",
      "Variable: year                 Importance: 0.0\n",
      "Variable: week_Fri             Importance: 0.0\n",
      "Variable: week_Mon             Importance: 0.0\n",
      "Variable: week_Sat             Importance: 0.0\n",
      "Variable: week_Sun             Importance: 0.0\n",
      "Variable: week_Thurs           Importance: 0.0\n",
      "Variable: week_Tues            Importance: 0.0\n",
      "Variable: week_Wed             Importance: 0.0\n",
      "Mean Absolute Error: 3.92 degrees.\n",
      "Accuracy: 93.76 %.\n",
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Best parrameters\n",
      "{'n_estimators': 800, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 50, 'bootstrap': False}\n",
      "After hyperparamater tuning:\n",
      "Mean Absolute Error: 3.67 degrees.\n",
      "Accuracy: 94.17 %.\n",
      "RMSE: 4.888652401005131\n"
     ]
    }
   ],
   "source": [
    "# Pandas is used for data manipulation\n",
    "import pandas as pd\n",
    "from   sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Read in data and display first 5 rows\n",
    "features = pd.read_csv('../datasets/temperatures.csv')\n",
    "\n",
    "# Show all columns.\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(features)\n",
    "\n",
    "# One-hot encode the data using pandas get_dummies\n",
    "features = pd.get_dummies(features)\n",
    "\n",
    "# Display the first 5 rows of the last 12 columns.\n",
    "print(features.head(5))\n",
    "\n",
    "# Use numpy to convert to arrays\n",
    "import numpy as np\n",
    "\n",
    "# Labels are the values we want to predict\n",
    "labels = np.array(features['actual'])\n",
    "\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "features= features.drop('actual', axis = 1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(features.columns)\n",
    "\n",
    "# Convert to numpy array\n",
    "features = np.array(features)\n",
    "\n",
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "\n",
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels)\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / test_labels)\n",
    "\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n",
    "\n",
    "# Print out the mean square error.\n",
    "mse = mean_squared_error(test_labels, predictions)\n",
    "print('RMSE:', np.sqrt(mse))\n",
    "\n",
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances\n",
    "for pair in feature_importances:\n",
    "    print('Variable: {:20} Importance: {}'.format(*pair))\n",
    "\n",
    "# New random forest with only the two most important variables\n",
    "rf_most_important = RandomForestRegressor(n_estimators= 1000, random_state=42)\n",
    "\n",
    "# Extract the two most important features\n",
    "important_indices = [feature_list.index('temp_1'), feature_list.index('average')]\n",
    "train_important = train_features[:, important_indices]\n",
    "test_important = test_features[:, important_indices]\n",
    "\n",
    "# Train the random forest\n",
    "rf_most_important.fit(train_important, train_labels)\n",
    "\n",
    "# Make predictions and determine the error\n",
    "predictions = rf_most_important.predict(test_important)\n",
    "errors = abs(predictions - test_labels)\n",
    "\n",
    "# Display the performance metrics\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "mape = np.mean(100 * (errors / test_labels))\n",
    "accuracy = 100 - mape\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation,\n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(train_features, train_labels)\n",
    "\n",
    "print(\"Best parrameters\")\n",
    "print(rf_random.best_params_)\n",
    "\n",
    "print(\"After hyperparamater tuning:\")\n",
    "rf_best = RandomForestRegressor(n_estimators=rf_random.best_params_['n_estimators'], min_samples_split=rf_random.best_params_['min_samples_split'],\n",
    "                                min_samples_leaf=rf_random.best_params_['min_samples_leaf'], max_features=rf_random.best_params_['max_features'],\n",
    "                                max_depth=rf_random.best_params_['max_depth'], bootstrap=rf_random.best_params_['bootstrap'])\n",
    "\n",
    "rf_best.fit(train_important, train_labels)\n",
    "\n",
    "# Make predictions and determine the error\n",
    "predictions = rf_best.predict(test_important)\n",
    "errors = abs(predictions - test_labels)\n",
    "\n",
    "# Display the performance metrics\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "mape = np.mean(100 * (errors / test_labels))\n",
    "accuracy = 100 - mape\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n",
    "# Print out the mean square error.\n",
    "mse = mean_squared_error(test_labels, predictions)\n",
    "print('RMSE:', np.sqrt(mse))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length  sepal width  petal length  petal width  species\n",
      "0           5.1          3.5           1.4          0.2        0\n",
      "1           4.9          3.0           1.4          0.2        0\n",
      "2           4.7          3.2           1.3          0.2        0\n",
      "3           4.6          3.1           1.5          0.2        0\n",
      "4           5.0          3.6           1.4          0.2        0\n",
      "Accuracy: 0.9777777777777777\n",
      "[2]\n",
      "Variable: petal length         Importance: 0.46\n",
      "Variable: petal width          Importance: 0.4\n",
      "Variable: sepal length         Importance: 0.12\n",
      "Variable: sepal width          Importance: 0.02\n",
      "Mean Absolute Error: 0.02 degrees.\n",
      "Accuracy: 96.77 %.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanleung/miniforge3/lib/python3.9/site-packages/sklearn/base.py:441: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "#Load dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Creating a DataFrame of given iris dataset.\n",
    "import pandas as pd\n",
    "data=pd.DataFrame({\n",
    "    'sepal length':iris.data[:,0],\n",
    "    'sepal width':iris.data[:,1],\n",
    "    'petal length':iris.data[:,2],\n",
    "    'petal width':iris.data[:,3],\n",
    "    'species':iris.target\n",
    "})\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "X=data[['sepal length', 'sepal width', 'petal length', 'petal width']]  # Features\n",
    "y=data['species']  # Labels\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "rf=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#Train the model using the training sets y_pred=rf.predict(X_test)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred=rf.predict(X_test)\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Predict species for a single flower.\n",
    "# sepal length = 3, sepal width = 5\n",
    "# petal length = 4, petal width = 2\n",
    "prediction = rf.predict([[3, 5, 4, 2]])\n",
    "# 'setosa', 'versicolor', 'virginica'\n",
    "print(prediction)\n",
    "\n",
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(X, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances\n",
    "for pair in feature_importances:\n",
    "    print('Variable: {:20} Importance: {}'.format(*pair))\n",
    "\n",
    "# New random forest with only the two most important variables\n",
    "rf_most_important = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Extract the two most important features\n",
    "important_cols = [feature_importances[x][0] for x in range(0, 2)]\n",
    "train_important = X_train[important_cols]\n",
    "test_important = X_test[important_cols]\n",
    "\n",
    "# Train the random forest\n",
    "rf_most_important.fit(train_important, y_train)\n",
    "\n",
    "# Make predictions and determine the error\n",
    "predictions = rf_most_important.predict(test_important)\n",
    "errors = abs(predictions - y_test)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Display the performance metrics\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "mape = np.mean(100 * (errors / y_test))\n",
    "accuracy = 100 - mape\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length  sepal width  petal length  petal width  species\n",
      "0           5.1          3.5           1.4          0.2        0\n",
      "1           4.9          3.0           1.4          0.2        0\n",
      "2           4.7          3.2           1.3          0.2        0\n",
      "3           4.6          3.1           1.5          0.2        0\n",
      "4           5.0          3.6           1.4          0.2        0\n",
      "\n",
      "**** Logistic Regression\n",
      "Accuracy: 0.9333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        15\n",
      "           1      1.000     0.800     0.889        15\n",
      "           2      0.833     1.000     0.909        15\n",
      "\n",
      "    accuracy                          0.933        45\n",
      "   macro avg      0.944     0.933     0.933        45\n",
      "weighted avg      0.944     0.933     0.933        45\n",
      "\n",
      "[2]\n",
      "\n",
      "**** Random Forest Regressor\n",
      "Accuracy: 0.9333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        15\n",
      "           1      0.929     0.867     0.897        15\n",
      "           2      0.875     0.933     0.903        15\n",
      "\n",
      "    accuracy                          0.933        45\n",
      "   macro avg      0.935     0.933     0.933        45\n",
      "weighted avg      0.935     0.933     0.933        45\n",
      "\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Creating a DataFrame of given iris dataset.\n",
    "import pandas as pd\n",
    "data=pd.DataFrame({\n",
    "    'sepal length':iris.data[:,0],\n",
    "    'sepal width':iris.data[:,1],\n",
    "    'petal length':iris.data[:,2],\n",
    "    'petal width':iris.data[:,3],\n",
    "    'species':iris.target\n",
    "})\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "X=data[['sepal length', 'sepal width', 'petal length', 'petal width']]  # Features\n",
    "y=data['species']  # Labels\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x            = StandardScaler()\n",
    "X_train_scaled  = sc_x.fit_transform(X_train)\n",
    "X_test_scaled = sc_x.transform(X_test)\n",
    "\n",
    "from sklearn              import metrics\n",
    "from sklearn.ensemble     import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def buildModelAndPredict(clf, X_train_scaled, X_test_scaled, y_train, y_test, title):\n",
    "    print(\"\\n**** \" + title)\n",
    "    #Train the model using the training sets y_pred=rf.predict(X_test)\n",
    "    clf_fit = clf.fit(X_train_scaled,y_train)\n",
    "    y_pred = clf_fit.predict(X_test_scaled)\n",
    "\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    # For explanation see:\n",
    "    # https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2\n",
    "    print(metrics.classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "    # Predict species for a single flower.\n",
    "    # sepal length = 3, sepal width = 5\n",
    "    # petal length = 4, petal width = 2\n",
    "    prediction = clf_fit.predict([[3, 5, 4, 2]])\n",
    "\n",
    "    # 'setosa', 'versicolor', 'virginica'\n",
    "    print(prediction)\n",
    "\n",
    "lr = LogisticRegression(fit_intercept=True, solver='liblinear')\n",
    "buildModelAndPredict(lr, X_train_scaled, X_test_scaled, y_train, y_test, \"Logistic Regression\")\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, max_features=3)\n",
    "buildModelAndPredict(rf, X_train_scaled, X_test_scaled, y_train, y_test, \"Random Forest Regressor\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}